{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "tLjg-4M5iNqX",
    "outputId": "d580b124-8c02-4f60-aced-9ffbbbb3513e"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done!!\n",
      "Query size:  100  /  Document size:  5000\n",
      "execution time(all small n): 0.000000\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "from zipfile import ZipFile\n",
    "file_name = \"test_data/2022-ntust-information-retrieval-hw2.zip\"\n",
    "\n",
    "with ZipFile(file_name, 'r',) as zip:\n",
    "    zip.extractall(path = 'test_data/extractHere/2022-ntust-information-retrieval-hw2')\n",
    "    print('Done!!')\n",
    "    All_lst = zip.namelist()\n",
    "\n",
    "start = time.time()\n",
    "Query_lst = []\n",
    "Doc_lst = []\n",
    "for i in All_lst:\n",
    "    if \"documents/\" in i:\n",
    "        Doc_lst.append(i)\n",
    "    elif \"queries/\" in i:\n",
    "        Query_lst.append(i)\n",
    "\n",
    "print(\"Query size: \", len(Query_lst),' / ',\"Document size: \", len(Doc_lst))\n",
    "\n",
    "end = time.time()\n",
    "print(\"execution time(all small n): %f\" %(end - start))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "pJdiEvWyzknj"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "number of terms:  262226\n",
      "avg_Doc_len:  1098.733\n",
      "execution time(all small n): 20.526171\n"
     ]
    }
   ],
   "source": [
    "# 可以在這建立lexicon\n",
    "lexicon = {}\n",
    "data_addr = 'test_data/extractHere/2022-ntust-information-retrieval-hw2/'\n",
    "\n",
    "start = time.time()\n",
    "for i in Query_lst:\n",
    "    file = open(data_addr + i, errors = 'ignore').read().split()\n",
    "    for term in file:\n",
    "        lexicon[term] = 0\n",
    "\n",
    "avg_Doc_len = 0\n",
    "# 要把document所有字也加進lexicon?\n",
    "for i in Doc_lst:\n",
    "    file = open(data_addr + i, errors = 'ignore').read().split()\n",
    "    for term in file:\n",
    "        avg_Doc_len = avg_Doc_len + 1\n",
    "        lexicon[term] = 0\n",
    "\n",
    "avg_Doc_len = avg_Doc_len/len(Doc_lst)\n",
    "\n",
    "print(\"number of terms: \", len(lexicon))\n",
    "print(\"avg_Doc_len: \", avg_Doc_len)\n",
    "\n",
    "end = time.time()\n",
    "print(\"execution time(all small n): %f\" %(end - start))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "T0CrrTt3VG8x",
    "outputId": "69c4524b-835b-4670-f3bd-365a2df1efde"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "execution time(all small n): 156.788369\n",
      "D_small_n:  262226\n",
      "avoid_zero:  262226\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "# 計算IDF中的n_i\n",
    "# Q_small_n = lexicon.copy()\n",
    "D_small_n = lexicon.copy()\n",
    "check = lexicon.copy()\n",
    "\n",
    "start = time.time()\n",
    "# for q in Query_lst:\n",
    "#     file = open(data_addr + q, errors = 'ignore').read().split()\n",
    "#     for term in check:\n",
    "#         check[term] = False\n",
    "    \n",
    "#     for term in file:\n",
    "#         if check[term] == False:\n",
    "#             Q_small_n[term] = Q_small_n[term] + 1 \n",
    "#             check[term] = True\n",
    "\n",
    "for d in Doc_lst:\n",
    "    file = open(data_addr + d, errors = 'ignore').read().split()\n",
    "    for term in check:\n",
    "        check[term] = False\n",
    "        \n",
    "    for term in file:\n",
    "        if term in lexicon and check[term] == False:\n",
    "            D_small_n[term] = D_small_n[term] + 1\n",
    "            check[term] = True\n",
    "\n",
    "end = time.time()\n",
    "print(\"execution time(all small n): %f\" %(end - start))\n",
    "\n",
    "# Q_small_n = list(Q_small_n.values())\n",
    "D_small_n = list(D_small_n.values())\n",
    "print(\"D_small_n: \", len(D_small_n))\n",
    "avoid_zero = np.ones(len(lexicon))\n",
    "print(\"avoid_zero: \", len(avoid_zero))\n",
    "\n",
    "# print(\"Q_small_n: \", Q_small_n)\n",
    "# print(\"D_small_n: \", D_small_n)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "6Dnqdvpd4C3B",
    "outputId": "c93850d1-c9d4-4123-b375-9c9765bda3e5"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "execution time(all D_tfidf): 181.342932\n"
     ]
    }
   ],
   "source": [
    "import math\n",
    "\n",
    "\n",
    "D_small_n_temp = D_small_n\n",
    "D_small_n = np.ones(len(D_small_n))* (len(lexicon)+ 0.5) - D_small_n\n",
    "D_small_n = np.divide(D_small_n, D_small_n_temp + np.ones(len(D_small_n))* 0.5)\n",
    "D_small_n = np.log10(D_small_n + avoid_zero)\n",
    "# D_small_n = (D_small_n + avoid_zero)\n",
    "\n",
    "# for i in D_small_n:\n",
    "#     i = math.log(i+1, 1.1)\n",
    "\n",
    "\n",
    "#每一迴圈計算一個document相對於所有lexicon的tf-idf，並存成一個list\n",
    "start = time.time()\n",
    "D_tfidf_lst = []\n",
    "\n",
    "for i in range(len(Doc_lst)):\n",
    "    D_k = lexicon.copy()\n",
    "    D_tfidf = [0 for _  in range(len(lexicon))]\n",
    "    \n",
    "    Doc_len = 0\n",
    "    file = open(data_addr + Doc_lst[i], errors = 'ignore').read().split()\n",
    "    for term in file:\n",
    "        Doc_len = Doc_len + 1\n",
    "        if term in lexicon:\n",
    "            D_k[term] = D_k[term] + 1\n",
    "\n",
    "# BM Match, K_1, b\n",
    "    K_1 = 1.7\n",
    "    S_1 = K_1 + 1\n",
    "    \n",
    "    b = 0.75\n",
    "    delta = 5\n",
    "    \n",
    "    K_3 = 1000\n",
    "    S_3 = K_3 + 1\n",
    "    \n",
    "    D_k = list(D_k.values())\n",
    "#     D_k = np.log2(D_k + avoid_zero)\n",
    "#     D_k = np.log10(D_k + avoid_zero)\n",
    "\n",
    "    D_k = np.divide(D_k, (1 - b + b* (Doc_len/ avg_Doc_len)))\n",
    "\n",
    "    D_k = D_k + np.ones(len(D_k))* delta\n",
    "    D_tfidf = np.multiply(S_1, D_k)\n",
    "    D_k = np.ones(len(D_k))* K_1 + D_k\n",
    "    D_tfidf = np.divide(D_tfidf, D_k)\n",
    "  \n",
    "    D_tfidf_lst.append(D_tfidf)\n",
    "end = time.time()\n",
    "print(\"execution time(all D_tfidf): %f\" %(end - start))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "gDpVoh2g-Cd6",
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "execution time(all Q_tfidf): 697.298992\n"
     ]
    }
   ],
   "source": [
    "import csv\n",
    "import winsound\n",
    "import time\n",
    "\n",
    "start = time.time()\n",
    "# 每一迴圈輸出一個Query的結果到test.csv\n",
    "with open(\"test_data/test.csv\", \"w\", newline='') as Answer_csv:\n",
    "    Answer_csv.truncate()\n",
    "    writer = csv.writer(Answer_csv)\n",
    "    writer.writerow(['Query', 'RetrievedDocuments'])\n",
    "    \n",
    "    for ans in range(len(Query_lst)):\n",
    "        Q_k = lexicon.copy()\n",
    "        Q_tfidf = [0 for _ in range(len(lexicon))]\n",
    "        sim_list = [0 for _ in range(len(Doc_lst))]\n",
    "\n",
    "        file = open(data_addr + Query_lst[ans], errors = 'ignore').read().split()\n",
    "        for term in file:\n",
    "            if term in lexicon:\n",
    "                Q_k[term] = Q_k[term] + 1\n",
    "\n",
    "                \n",
    "        Q_k = list(Q_k.values())\n",
    "        \n",
    "# BM Match, Query weighting.    \n",
    "#         Q_k = np.log10(Q_k + avoid_zero)\n",
    "#         for i in Q_k:\n",
    "#             i = math.log(i+1, 100)\n",
    "        \n",
    "        Q_tfidf = np.multiply(S_3, Q_k)\n",
    "        Q_k = np.ones(len(Q_k))* K_3 + Q_k\n",
    "        Q_tfidf = np.divide(Q_tfidf, Q_k)\n",
    "        \n",
    "        for i in range(len(D_tfidf_lst)):\n",
    "            sim_temp = np.multiply(Q_tfidf, D_tfidf_lst[i])\n",
    "            sim_temp = np.multiply(sim_temp, D_small_n)\n",
    "            sim_list[i] = np.sum(sim_temp)\n",
    "    \n",
    "        sorted_Doc = sorted(range(len(sim_list)), key=lambda k: sim_list[k], reverse = True)\n",
    "\n",
    "        test_str = Doc_lst[sorted_Doc[0]].replace('documents/', '').replace(\".txt\", '')\n",
    "        for i in range(1, len(Doc_lst)): \n",
    "            test_str = test_str + ' ' + Doc_lst[sorted_Doc[i]].replace('documents/', '').replace(\".txt\", '')\n",
    "    \n",
    "        writer.writerow([Query_lst[ans].replace('queries/', '').replace(\".txt\", '')] + [test_str])\n",
    "\n",
    "end = time.time()\n",
    "print(\"execution time(all Q_tfidf): %f\" %(end - start))\n",
    "\n",
    "duration = 500  # millisecond\n",
    "freq = 440  # Hz\n",
    "for i in range(5):\n",
    "    winsound.Beep(freq, duration)"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
