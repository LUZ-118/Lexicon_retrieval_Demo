{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "tLjg-4M5iNqX",
    "outputId": "d580b124-8c02-4f60-aced-9ffbbbb3513e"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done!!\n"
     ]
    }
   ],
   "source": [
    "from zipfile import ZipFile\n",
    "file_name = \"test_data/2022-ntust-information-retrieval-hw1.zip\"\n",
    "with ZipFile(file_name, 'r',) as zip:\n",
    "    zip.extractall(path = 'test_data/extractHere/2022-ntust-information-retrieval-hw1')\n",
    "    print('Done!!')\n",
    "    All_lst = zip.namelist()\n",
    "\n",
    "Query_lst = []\n",
    "Doc_lst = []\n",
    "for i in All_lst:\n",
    "    if \"documents/\" in i:\n",
    "        Doc_lst.append(i)\n",
    "    elif \"queries/\" in i:\n",
    "        Query_lst.append(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "pJdiEvWyzknj"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "190\n"
     ]
    }
   ],
   "source": [
    "# 可以在這建立lexicon\n",
    "lexicon = {}\n",
    "data_addr = 'test_data/extractHere/2022-ntust-information-retrieval-hw1/'\n",
    "\n",
    "for i in Query_lst:\n",
    "    file = open(data_addr + i, errors = 'ignore').read().split()\n",
    "    for term in file:\n",
    "        lexicon[term] = 0\n",
    "\n",
    "# 要把document所有字也加進lexicon?\n",
    "# for i in Doc_lst:\n",
    "#     file = open(\"test_data/extractHere/\" + i, errors = 'ignore').read().split()\n",
    "#     for term in file:\n",
    "#         lexicon[term] = 0\n",
    "\n",
    "print(len(lexicon))\n",
    "# print(lexicon)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "T0CrrTt3VG8x",
    "outputId": "69c4524b-835b-4670-f3bd-365a2df1efde"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "execution time(all small n): 3.963565\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "\n",
    "# 計算IDF中的n_i\n",
    "Q_small_n = lexicon.copy()\n",
    "D_small_n = lexicon.copy()\n",
    "check = lexicon.copy()\n",
    "# for term in check:\n",
    "#     check[term] = False\n",
    "\n",
    "start = time.time()\n",
    "for q in Query_lst:\n",
    "    file = open(data_addr + q, errors = 'ignore').read().split()\n",
    "    for term in check:\n",
    "        check[term] = False\n",
    "    \n",
    "    for term in file:\n",
    "        if check[term] == False:\n",
    "            Q_small_n[term] = Q_small_n[term] + 1 \n",
    "            check[term] = True\n",
    "\n",
    "for d in Doc_lst:\n",
    "    file = open(data_addr + d, errors = 'ignore').read().split()\n",
    "    for term in check:\n",
    "        check[term] = False\n",
    "        \n",
    "    for term in file:\n",
    "        if term in lexicon and check[term] == False:\n",
    "            D_small_n[term] = D_small_n[term] + 1\n",
    "            check[term] = True\n",
    "\n",
    "end = time.time()\n",
    "print(\"execution time(all small n): %f\" %(end - start))\n",
    "\n",
    "Q_small_n = list(Q_small_n.values())\n",
    "D_small_n = list(D_small_n.values())\n",
    "\n",
    "# print(\"Q_small_n: \", Q_small_n)\n",
    "# print(\"D_small_n: \", D_small_n)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "6Dnqdvpd4C3B",
    "outputId": "c93850d1-c9d4-4123-b375-9c9765bda3e5"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "execution time(all D_tfidf): 0.404155\n"
     ]
    }
   ],
   "source": [
    "import math\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "#每一迴圈計算一個document相對於所有lexicon的tf-idf，並存成一個list\n",
    "start = time.time()\n",
    "D_tfidf_lst = []\n",
    "\n",
    "for i in range(len(Doc_lst)):\n",
    "    D_k = lexicon.copy()\n",
    "    D_tfidf = [0 for _  in range(len(lexicon))]\n",
    "  \n",
    "    file = open(data_addr + Doc_lst[i], errors = 'ignore').read().split()\n",
    "    for term in file:\n",
    "        if term in lexicon:\n",
    "            D_k[term] = D_k[term] + 1\n",
    "\n",
    "            \n",
    "# document TF-IDF option, D_k = times, D_tfidf = weights\n",
    "#     D_tfidf[j] = D_k[j] * math.log(len(Doc_lst) / D_small_n[j])\n",
    "#     D_tfidf[j] = 1 + D_k[j]\n",
    "#     D_tfidf[j] = (1 + D_k[j]) * math.log(len(Doc_lst) / D_small_n[j])\n",
    "    D_k = list(D_k.values())\n",
    "    D_tfidf = np.multiply(np.log10(np.ones(len(D_k)) + D_k), np.log10(np.divide(np.ones(len(D_k))*len(Doc_lst), np.ones(len(D_k)) +D_small_n))\n",
    ")\n",
    "  \n",
    "    D_tfidf_lst.append(D_tfidf)\n",
    "end = time.time()\n",
    "print(\"execution time(all D_tfidf): %f\" %(end - start))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "gDpVoh2g-Cd6"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "execution time(all Q_tfidf): 0.825123\n"
     ]
    }
   ],
   "source": [
    "import csv\n",
    "\n",
    "\n",
    "start = time.time()\n",
    "# 每一迴圈輸出一個Query的結果到test.csv\n",
    "with open(\"test_data/test.csv\", \"w\", newline='') as Answer_csv:\n",
    "    writer = csv.writer(Answer_csv)\n",
    "    writer.writerow(['Query', 'RetrievedDocuments'])\n",
    "    \n",
    "#     for item in Q_small_n:\n",
    "#         if item < len(Query_lst):\n",
    "#             item = item+1\n",
    "    for ans in range(len(Query_lst)):\n",
    "        Q_k = lexicon.copy()\n",
    "        Q_tfidf = [0 for _ in range(len(lexicon))]\n",
    "        sim_list = [0 for _ in range(len(Doc_lst))]\n",
    "\n",
    "        file = open(data_addr + Query_lst[ans], errors = 'igonore').read().split()\n",
    "        for term in file:\n",
    "            if term in lexicon:\n",
    "                Q_k[term] = Q_k[term] + 1\n",
    "\n",
    "    \n",
    "#     Query TF-IDF option, Q_k = times, Q_tfidf = weights\n",
    "#         Q_tfidf[i] = (0.5 + 0.5*(Q_k[j]/max(Q_k))) * math.log(len(Query_lst) / Q_small_n[j])\n",
    "#         Q_tfidf[i] = math.log(1 + len(Query_lst) / Q_small_n[i])\n",
    "#         Q_tfidf[i] = (1 + Q_k[i]) * math.log(len(Query_lst) / Q_small_n[i])\n",
    "#         if Q_small_n[term] < len(Query_lst):\n",
    "#             Q_tfidf[term] = math.log(1 + Q_k[term]) * math.log(len(Query_lst) / (Q_small_n[term] + 1))\n",
    "#         else:\n",
    "#             Q_tfidf[term] = math.log(1 + Q_k[term]) * math.log(len(Query_lst) / (Q_small_n[term]))\n",
    "#         Q_k = list(Q_k.values())\n",
    "#         Q_tfidf = np.multiply(np.log10(np.ones(len(Q_k)) + Q_k), np.log2(np.divide(np.ones(len(Q_k))*len(Query_lst), Q_small_n)))\n",
    "#         Q_tfidf = np.multiply(np.log(np.ones(len(Q_k)) + Q_k), np.log(np.divide(np.ones(len(Q_k))*len(Query_lst), Q_small_n+np.ones(len(lexicon)))))\n",
    "        \n",
    "        Q_k = list(Q_k.values())\n",
    "        Q_k_temp = Q_k.copy()\n",
    "        Q_k_temp = Q_k_temp + np.ones(len(Q_k))\n",
    "        for i in range(len(Q_k_temp)):\n",
    "            Q_k_temp[i] = math.log(Q_k_temp[i], 100)\n",
    "        Q_tfidf = np.multiply((Q_k_temp), np.log10(np.divide(np.ones(len(Q_k))*len(Query_lst), Q_small_n)))\n",
    "        \n",
    "        \n",
    "        for i in range(len(D_tfidf_lst)):\n",
    "            sim_list[i] = np.sum(np.multiply(Q_tfidf, D_tfidf_lst[i])) / ( math.sqrt(np.sum(np.multiply(Q_tfidf, Q_tfidf))) * math.sqrt(np.sum(np.multiply(D_tfidf_lst[i], D_tfidf_lst[i])))+1)\n",
    "    \n",
    "        sorted_Doc = sorted(range(len(sim_list)), key=lambda k: sim_list[k], reverse=True)\n",
    "\n",
    "        test_str = Doc_lst[sorted_Doc[0]].replace('documents/', '').replace(\".txt\", '')\n",
    "        for i in range(1, len(Doc_lst)): \n",
    "            test_str = test_str + ' ' + Doc_lst[sorted_Doc[i]].replace('documents/', '').replace(\".txt\", '')\n",
    "    \n",
    "        writer.writerow([Query_lst[ans].replace('queries/', '').replace(\".txt\", '')] + [test_str])\n",
    "\n",
    "end = time.time()\n",
    "print(\"execution time(all Q_tfidf): %f\" %(end - start))"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
